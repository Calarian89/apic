{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from requests.exceptions import SSLError\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Suppress unverified HTTPS warnings, but keep others\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "warnings.filterwarnings(\"ignore\", \".*Unverified HTTPS.*\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class APICClient:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.username, self.password = self.load_api_credentials()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config(config_path):\n",
    "        return toml.load(config_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_api_credentials():\n",
    "        username = os.getenv(\"API_USERNAME\")\n",
    "        password = os.getenv(\"API_PASSWORD\")\n",
    "        return username, password\n",
    "\n",
    "    def login(self, environment):\n",
    "        api_base_url = self.config[\"api\"][environment]\n",
    "        login_endpoint = self.config[\"endpoint\"][\"login\"]\n",
    "        login_url = f\"{api_base_url}{login_endpoint}\"\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        try:\n",
    "            response = session.post(\n",
    "                login_url,\n",
    "                json={\n",
    "                    \"aaaUser\": {\n",
    "                        \"attributes\": {\"name\": self.username, \"pwd\": self.password}\n",
    "                    }\n",
    "                },\n",
    "                verify=False,\n",
    "            )\n",
    "        except (Exception, SSLError) as e:\n",
    "            return None, f\"Login error: {e}\"\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return (\n",
    "                None,\n",
    "                f\"Failed to authenticate with the APIC. Status code: {response.status_code}\",\n",
    "            )\n",
    "\n",
    "        login_data = response.json()\n",
    "        token = login_data[\"imdata\"][0][\"aaaLogin\"][\"attributes\"][\"token\"]\n",
    "        session.cookies.set(\"APIC-cookie\", token)\n",
    "\n",
    "        return session, {\"APIC-cookie\": token}\n",
    "\n",
    "    def fetch_data(self, session, cookie, environment, endpoint_key, **url_params):\n",
    "        if session is None:\n",
    "            return []\n",
    "\n",
    "        api_base_url = self.config[\"api\"][environment]\n",
    "        api_endpoint = self.config[\"endpoint\"][endpoint_key]\n",
    "        url = f\"{api_base_url}{api_endpoint.format(**url_params)}\"\n",
    "\n",
    "        response = session.get(url, cookies=cookie, verify=False)\n",
    "        response_json = response.json()\n",
    "\n",
    "        return response_json.get(\"imdata\", [])\n",
    "\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def get_active_nodes(self, session, cookie, environment, pod):\n",
    "        nodes_json = self.client.fetch_data(\n",
    "            session, cookie, environment, \"node\", pod=pod\n",
    "        )\n",
    "\n",
    "        node_list = []\n",
    "        node_name_dict = {}\n",
    "\n",
    "        for node_item in nodes_json:\n",
    "            fabric_node = node_item.get(\"fabricNode\")\n",
    "            if fabric_node and fabric_node[\"attributes\"][\"adSt\"] == \"on\":\n",
    "                leaf_dn = fabric_node[\"attributes\"][\"id\"]\n",
    "                node_name = fabric_node[\"attributes\"][\"name\"]\n",
    "                node_name_dict[leaf_dn] = node_name\n",
    "                node_list.append(leaf_dn)\n",
    "\n",
    "        node_list.sort()\n",
    "        return node_list, node_name_dict\n",
    "\n",
    "    def get_memory(self, session, cookie, environment, pod, leaf_dn):\n",
    "        memory_data = self.client.fetch_data(\n",
    "            session, cookie, environment, \"memory\", pod=pod, leaf_dn=leaf_dn\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            attributes = (\n",
    "                memory_data[0].get(\"procSysMemHist5min\", {}).get(\"attributes\", {})\n",
    "            )\n",
    "            used_avg = int(attributes.get(\"usedAvg\", 0))\n",
    "            total_avg = int(attributes.get(\"totalAvg\", 0))\n",
    "            if total_avg == 0:\n",
    "                total_capacity_percentage = 0\n",
    "            else:\n",
    "                total_capacity_percentage = (used_avg / total_avg) * 100\n",
    "        except KeyError:\n",
    "            print(\n",
    "                f\"Key 'procSysMemHist5min' not found in memory data for pod {pod}, leaf_dn {leaf_dn}\"\n",
    "            )\n",
    "            used_avg, total_avg, total_capacity_percentage = 0, 0, 0\n",
    "\n",
    "        return {\n",
    "            \"used_avg\": used_avg,\n",
    "            \"total_avg\": total_avg,\n",
    "            \"total_capacity_percentage\": total_capacity_percentage,\n",
    "        }\n",
    "\n",
    "    def get_cpu(self, session, cookie, environment, pod, leaf_dn):\n",
    "        cpu_data = self.client.fetch_data(\n",
    "            session, cookie, environment, \"cpus\", pod=pod, leaf_dn=leaf_dn\n",
    "        )\n",
    "\n",
    "        if not cpu_data:\n",
    "            return {\"cpu_kernel\": \"\", \"cpu_user\": \"\"}\n",
    "\n",
    "        try:\n",
    "            attributes = cpu_data[0].get(\"procSysCPUHist5min\", {}).get(\"attributes\", {})\n",
    "            if not attributes:\n",
    "                print(f\"No CPU data available for pod {pod}, leaf_dn {leaf_dn}\")\n",
    "                return {\"cpu_kernel\": \"\", \"cpu_user\": \"\"}\n",
    "\n",
    "            return {\n",
    "                \"cpu_kernel\": attributes.get(\"kernelAvg\", \"\"),\n",
    "                \"cpu_user\": attributes.get(\"userAvg\", \"\"),\n",
    "            }\n",
    "        except (requests.exceptions.RequestException, KeyError) as e:\n",
    "            print(f\"Error retrieving CPU data for pod {pod}, leaf_dn {leaf_dn}: {e}\")\n",
    "            return {\"cpu_kernel\": \"\", \"cpu_user\": \"\"}\n",
    "\n",
    "    def get_tcam(self, session, cookie, environment, pod, leaf_dn):\n",
    "        tcam_data = self.client.fetch_data(\n",
    "            session, cookie, environment, \"tcam\", pod=pod, leaf_dn=leaf_dn\n",
    "        )\n",
    "\n",
    "        if not tcam_data:\n",
    "            return {\n",
    "                \"tcam_current_util\": \"\",\n",
    "                \"tcam_max_capacity\": \"\",\n",
    "                \"total_tcam_percentage\": \"\",\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            attributes = (\n",
    "                tcam_data[0].get(\"eqptcapacityPolUsage5min\", {}).get(\"attributes\", {})\n",
    "            )\n",
    "            if not attributes:\n",
    "                print(f\"No TCAM data available for pod {pod}, leaf_dn {leaf_dn}\")\n",
    "                return {\n",
    "                    \"tcam_current_util\": \"\",\n",
    "                    \"tcam_max_capacity\": \"\",\n",
    "                    \"total_tcam_percentage\": \"\",\n",
    "                }\n",
    "\n",
    "            tcam_usage = int(attributes[\"polUsageCum\"]) - int(\n",
    "                attributes[\"polUsageBase\"]\n",
    "            )\n",
    "            pol_usage_cap_cum = int(attributes[\"polUsageCapCum\"])\n",
    "\n",
    "            if pol_usage_cap_cum > 0:\n",
    "                total_capacity_percentage = (tcam_usage / pol_usage_cap_cum) * 100\n",
    "            else:\n",
    "                total_capacity_percentage = 0\n",
    "\n",
    "            return {\n",
    "                \"tcam_current_util\": attributes[\"polUsageCum\"],\n",
    "                \"tcam_max_capacity\": pol_usage_cap_cum,\n",
    "                \"total_tcam_percentage\": total_capacity_percentage,\n",
    "            }\n",
    "        except (requests.exceptions.RequestException, KeyError) as e:\n",
    "            print(f\"Error retrieving TCAM data for pod {pod}, leaf_dn {leaf_dn}: {e}\")\n",
    "            return {\n",
    "                \"tcam_current_util\": \"\",\n",
    "                \"tcam_max_capacity\": \"\",\n",
    "                \"total_tcam_percentage\": \"\",\n",
    "            }\n",
    "\n",
    "    def get_lpm(self, session, cookie, environment, pod, leaf_dn):\n",
    "        lpm_data = self.client.fetch_data(\n",
    "            session,\n",
    "            cookie,\n",
    "            environment,\n",
    "            \"lpm\",\n",
    "            pod=pod,\n",
    "            leaf_dn=leaf_dn,\n",
    "        )\n",
    "\n",
    "        # check if lpm_data is empty\n",
    "        if not lpm_data:\n",
    "            # set the values in data_dict to blank strings\n",
    "            data_dict = {\n",
    "                \"lpm_current_util\": \"\",\n",
    "                \"lpm_max_capacity\": \"\",\n",
    "            }\n",
    "        else:\n",
    "            try:\n",
    "                # extract the attributes object\n",
    "                attributes = lpm_data[0][\"eqptcapacityPrefixEntries5min\"][\"attributes\"]\n",
    "                lpm_current_util = int(attributes[\"extNormalizedAvg\"])\n",
    "                lpm_max_capacity = int(attributes[\"extNormalizedMax\"])\n",
    "\n",
    "                # extract the fields of interest and store them in a dictionary\n",
    "                data_dict = {\n",
    "                    \"lpm_current_util\": lpm_current_util,\n",
    "                    \"lpm_max_capacity\": lpm_max_capacity,\n",
    "                }\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # set the values in data_dict to blank strings\n",
    "                data_dict = {\n",
    "                    \"lpm_current_util\": \"\",\n",
    "                    \"lpm_max_capacity\": \"\",\n",
    "                }\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "def get_switch_data():\n",
    "    client = APICClient(\"api.toml\")\n",
    "    collector = DataCollector(client)\n",
    "\n",
    "    environments = [\"lab\", \"prod\"]\n",
    "    pods = {1: \"19thST\", 2: \"ESF\"}\n",
    "\n",
    "    login_errors = []\n",
    "    current_datetime = datetime.now()\n",
    "    apic_data = []\n",
    "\n",
    "    for environment in environments:\n",
    "        login_error = False\n",
    "        error_msg = \"\"\n",
    "        session = None\n",
    "\n",
    "        try:\n",
    "            session, cookie = client.login(environment)\n",
    "        except Exception as e:\n",
    "            print(f\"Login error: {e}\")\n",
    "            login_error = True\n",
    "            error_msg = str(e)\n",
    "\n",
    "        if not session:\n",
    "            login_error = True\n",
    "            error_msg = cookie\n",
    "\n",
    "        login_errors.append((environment, login_error, error_msg))\n",
    "\n",
    "        if login_error:\n",
    "            continue\n",
    "\n",
    "        for pod, location in pods.items():\n",
    "            active_nodes, node_name_dict = collector.get_active_nodes(\n",
    "                session, cookie, environment, pod\n",
    "            )\n",
    "\n",
    "            for leaf_dn in active_nodes:\n",
    "                node_name = node_name_dict[leaf_dn]\n",
    "\n",
    "                tcam_dict = collector.get_tcam(\n",
    "                    session, cookie, environment, pod, leaf_dn\n",
    "                )\n",
    "                cpu_dict = collector.get_cpu(session, cookie, environment, pod, leaf_dn)\n",
    "                memory_dict = collector.get_memory(\n",
    "                    session, cookie, environment, pod, leaf_dn\n",
    "                )\n",
    "                lpm_dict = collector.get_lpm(session, cookie, environment, pod, leaf_dn)\n",
    "\n",
    "                row_dict = {\n",
    "                    \"date_time\": current_datetime,\n",
    "                    \"environment\": environment,\n",
    "                    \"location\": location,\n",
    "                    \"node_name\": node_name,\n",
    "                    **tcam_dict,\n",
    "                    **cpu_dict,\n",
    "                    **memory_dict,\n",
    "                    **lpm_dict,\n",
    "                }\n",
    "                apic_data.append(row_dict)\n",
    "\n",
    "        if session:\n",
    "            session.close()\n",
    "\n",
    "    df = pd.DataFrame(apic_data)\n",
    "\n",
    "    # Ensure consistent data types for all numeric columns\n",
    "    numeric_columns = [\n",
    "        \"used_avg\",\n",
    "        \"total_avg\",\n",
    "        \"total_capacity_percentage\",\n",
    "        \"tcam_current_util\",\n",
    "        \"tcam_max_capacity\",\n",
    "        \"total_tcam_percentage\",\n",
    "        \"cpu_kernel\",\n",
    "        \"cpu_user\",\n",
    "        \"lpm_current_util\",\n",
    "        \"lpm_max_capacity\",\n",
    "    ]\n",
    "    for column in numeric_columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors=\"coerce\").astype(float)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    # Convert Pandas DataFrame to Spark DataFrame without Arrow optimization\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark.conf.set(\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\", \"true\"\n",
    "    )  # Revert to the default setting\n",
    "\n",
    "    # Save the Spark DataFrame to a Parquet file with overwrite mode\n",
    "    output_parquet_path = \"/mnt/edl/raw/iit_apic_raw/TCAM.parquet\"\n",
    "    spark_df.write.mode(\"overwrite\").parquet(output_parquet_path)\n",
    "\n",
    "    if login_errors:\n",
    "        return df, login_errors\n",
    "    else:\n",
    "        return df, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_switch_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
